\chapter{Design and Implementation}
\label{cha:Design}

\section{Platform and Programming Language}
\label{sec:DesignLanguage}
As this project aims to produce visualisations of the internet embedded in hyperbolic space it must employ some existing technologies to do so. This section will discuss which technologies will be used and will justify their use. 

The most important technical decision to be made is the choice of programming language as it affects how the project will be designed and limits possible techniques. The following languages will be considered for use in this project:

\begin{itemize}
	\item{MATLAB}
	\item{Python}
	\item{Java}
\end{itemize}

\subsection{MATLAB}
MATLAB \cite{MATLAB:2015} is a high level mathematical programming language which is provided with an environment. It excels at mathematical programming as it provides many high level functions designed to perform calculations. MATLAB provides matrix and vector support as standard. As this project will require some complex mathematical operations it would seem that MATLAB is the ideal choice, however it does have limitations which affect its viability. 

MATLAB is a commercial product, and is not free, requiring payment in order to use the software. The University of York does provide MATLAB on university owned PC's, however it is currently not possible to install this provided version on a personal PC, where most of the work for this project will be undertaken. Additionally, MATLAB has relatively few third party libraries and if functionality for a specific task does not exist in the language it would most likely be necessary to develop it from scratch.  It is unlikely that MATLAB will have the necessary third party libraries for collecting the data to be used in this project. Due to these limitations it would seem that MATLAB is not a suitable candidate for this project.

\subsection{Python}
Python (\textit{www.python.org}) is a multi-platform programming language which boasts one of the largest sets of third party libraries of any language. This, combined with the speed of its native C implementation makes it an ideal choice for this project. Python comes pre-installed on many operating systems and is available on all popular platforms.

Unlike MATLAB, python does not provide any mathematical matrix or vector support as standard, but does provide array functionality through "lists". Fortunately, as python is so widely used it has many third party libraries which provide mathematical functions, such as NumPy \cite{python_numpy_2015} and SciPy \cite{jones_scipy:_2001}. These libraries provide functionality similar to MATLAB while still allowing the flexibility of a general programming language.

\subsection{Java}
Java (\textit{www.java.com}) is a multi-platform programming language which compiles programs to run on a virtual machine known as the JVM. Java has wide support with many third party libraries and runs on many platforms. As all code is executed on the JVM, programs need only be compiled once and will run on all platforms for which a version of the JVM exists. 

Though Java is natively multi platform, this feature is not actually useful in the scope of this project as it will only be run on a single platform. Additionally, as compiled Java code is interpreted by the JVM and is not native machine code, the general performance of the language falls below native languages.

Due to the limitations of Java and MATLAB this project will be written in python, making use of NumPy, SciPy and any other third party libraries which may be necessary.

\section{Data Sources}
\label{sec:DesignDataSources}
As the primary aim of this project is to create and visualise an embedding of the internet it is necessary to collect data describing its structure, on which to base the embedding. Using Autonomous Systems as the subject of the mapping serves to make data collection easier, however the following data is required for each system if an embedding is to be created:

\begin{itemize}
	\item{Organisation and ID}
	\item{Relationships between Autonomous Systems}
	\item{Location}
\end{itemize}

\subsection{Organisation and ID}
Though it may seem trivial, finding a dataset which contains a mapping of Autonomous System Numbers (ASNs) to the organisation that owns them was not straightforward. At first attempt, the use of the program \textit{whois} was considered as it is possible to query it with an ASN in order to retrieve the owner as well as further data which could have been used to identify relationships between ASes. However, upon testing several queries it became clear that the information provided by \textit{whois} was not consistent across RIRs. Further, executing an external program, which requires an internet connection, for each AS would add a large amount of overhead to the application. 

If it was not possible to connect to the internet to retrieve the data for each AS individually then a pre-compiled dataset is required. Of course, if a database could be found that contains all required data then the choice would be simple, however no such database exists. Instead, each piece of data described earlier must be obtained from separate databases. 

In order to begin enumerating ASes a dataset which contains the ASNs for all systems must be used. There are several web pages (for example \textit{bgp.potaroo.net/cidr/autnums.html}) which simply list all known ASNs along with their organisation, however the system through which this data is collected is unclear and the formatting of the web pages makes them difficult to parse. 

Clearly, some form of downloadable database is required which would allow the application to have fast access to all available AS data. One such dataset is provided by the Center for Applied Internet Data Analysis (CAIDA) and contains several different files which provide both ASN and organisation data \cite{org_mapping_caida_2015}. Further, this data is provided for free to educational users. The files themselves are simply text files which can be parsed easily in Python. The method through which the data is collected is clearly outlined on the website as well as how often new data is published. This dataset provides a reliable way of enumerating ASes and finding to which organisation they belong.

\subsection{Relationships between Autonomous Systems}
Without data describing the relationships between ASes it would not be possible to create an embedding. Collecting this data will allow the physical connections between ASes to be determined, which, when used in conjunction with location data, provides enough information to calculate the distances between connected ASes.

Again, use of the \textit{whois} application was considered but the flaws of this method definitely outweigh its advantages. Again, CAIDA provides a convenient dataset named \textit{AS Relationships} \cite{relationships_caida_2015} which contains the inferred relationships between ASes where each relationship represents a physical connection. This dataset is provided free for educational use and contains a text file which can be easily parsed in Python. 

\subsection{Location}
Determining the geographical location of an AS provides a way to determine distances between ASes, an operation which is required in order to create an embedding. Upon searching for a dataset which contained the locations of all ASes it became apparent that no such dataset existed. This was primarily because an AS does not have a single location, it is possible for an AS to span a very large geographic area. This issue caused a significant setback in the projects design, how would it be possible to retrieve location data for an entity which does not have a singular location?

When considering the solution to this problem it became apparent that in order to retrieve location data for an AS it needed to have a component for which a location could be determined. Physical internet components such as routers have a defined location, and each component can be identified by its IP address.

Having determined that it is possible to determine a location for an IP address the next step is to find an IP address for an AS. This is not as simple as using a service to perform a lookup as there are no providers for this type of data. There is, however, a Python module which provides IP to ASN lookups known as pyasn \cite{asghari_pyasn_2014}. Though this may not seem useful it is possible to use the module to perform a reverse lookup for all known IP addresses in a specific AS. This module operates by providing functions to parse the current BGP table, converting it into a format which can be read by pyasn. This data is then used to perform lookups for which IP addresses route to which ASN.

Once this list of IP addresses is obtained it is then possible to determine the location of one of them. This can be done through the use of an IP geolocation service. Python bindings exist for many of these services, of which MaxMinds GeoIP \cite{python_geoip_2014} is one. This library contains functionality for downloading a database ahead of time for fast access. As all of the data required for this project is available locally on disk, the initial start up of the application should be significantly faster. 

\section{Further Data Considerations}

This section aims to outline any further data considerations which were raised during development of this project.

\subsection{Dataset Size}

Upon inspecting the datasets chosen for this project it became apparent that volume of data available was very large. Due to the calculations that must be performed in order to create an embedding a very large amount of memory is required. This is not feasible as access to a machine with enough RAM is not possible, therefore one of two workarounds must be employed: either store the data somewhere other than RAM or reduce the size of the data. 

The specific point in the application at which the data becomes very large will occur when calculating the distance matrix for all ASes. This matrix will be square with dimensions equal to the number of ASes. Attempts must be made to reduce the size of this matrix or the aims of the project cannot be fulfilled.

Storing the data outside of RAM would seem difficult in this case, but it is possible. In order to implement such a system in Python, a technology called Hierarchical Data Format (HDF) can be used. This allows for the creation of disk backed variable storage within Python that acts as though the object is stored in RAM. Access to the data would of course be slower than traditional variables but RAM is no longer a constraint. However, as the data being stored is a matrix it must support all of the numpy matrix operators. While the Python bindings for HDF (h5py) do support numpy, they do not support performing matrix operations on data backed by HDF. HDF is not a viable candidate to solve this problem for this reason.

While moving the data out of RAM may seem like a convenient solution, it is impossible to maintain compatibility with numpy while performing such an abstraction. Instead it would be easier to attempt to minimise or compress the data.The scipy library contains a \textit{sparse} matrix class which is effectively compressed and performs very well on matrices which contain many occurrences of the same value. Unfortunately, it is extremely inefficient to modify such a matrix heavily once it has been created which is a feature required for the construction of an embedding. 

Having exhausted all other lines of enquiry it is necessary to consider using a smaller dataset than initially intended. Unfortunately, this is not as simple as only using the first 1000 ASes, for example, as it is possible (though unlikely) for no relationships to exist between any of those ASes. Instead, the subset must be chosen carefully so as to eliminate any ASes which have zero links to the rest of the subset. One system which could be used to achieve this would be to follow the links from a specific AS to a set depth and use all ASes found as a subset. This could be implemented recursively and would provide a consistent way to generate small subsets of the main dataset.

\subsection{Data Availability}
While the chosen data sources contain relatively complete data for most ASes it is impossible to have full coverage. Of particular note is the unavailability of an IP address for some ASes. This is due to these systems having no IP address ranges which route to them, which is fairly common. Unfortunately, if it is not possible to determine a valid IP address for an AS it is not possible to determine its location. If an AS has no known location it cannot be part of the embedding as its distance from other ASes cannot be calculated. 

For this reason, it is necessary to validate the dataset before it is processed by removing any AS for which there is incomplete data. While this may seem an extreme measure there is no alternative as any ASes which have missing data are useless in this context. Further, performing this validation before the data subset is generated means that any ASes which are eliminated cannot be followed by the recursive algorithm. This effectively prevents any ASes which may have been orphaned from being included in the embedding, which prevents misleading information from being shown in the visualisations. 

\subsection{Organisation of Data}
As each AS has a large amount of associated data, it will be necessary to produce a method of organising this data so that it can be used effectively. It may be possible to simply encode the data for each AS into a list, and implement a rule for extracting data from these lists. For example, the ASN is at index $0$ and the name at index $1$. Unfortunately, this solution does not scale well when each object contains numerous data. Further, ASes are identified by their ASN and python lists by an index beginning at zero, these two indexing systems are not compatible as gaps exist in the allocation of ASNs. 

It seems that there are two separate problems here which must be solved in conjunction. Firstly, the issue of storing the data for each AS; a sensible way to solve this would be to define a custom python class which holds all of the data for a single AS. It is then possible to easily extract a specific piece of AS data from an AS for which the class object is known. 

The second problem, regarding the indexing of ASes is also simple to solve. Python has a built in dictionary structure \textit{dict} which allows for the referencing of its contents through a hashable key. Storing each AS class object in a dict with the ASN as the key would allow any AS to be located and any piece of data for that AS to be extracted. It is important to note that the ASN is being stored as a string in this instance, as it has no numerical meaning and arithmetic should not be performed on it. 

Though most of the data associated with an AS is singular, the location or name for example, there is a special case in the connections between ASes. As the network of ASes can be represented as a graph this structure must be described by the stored data. A simple way to achieve this is to store a list of the ASN for all ASes connected to an AS. This method provides a way to retrieve all connections to an AS easily, by simply extracting the list and a way of following these connections through the main AS dict, using the ASNs. If, however any more information was to be stored about a particular AS to AS relationship, such as whether that link is peer-peer or provider-customer, then a new \textit{peering} class can be created which facilitates the storage of such data. 


\section{Performing The Embedding}

The method discussed previously in section \ref{sec:hyperbolic_embedding} will be used to perform the embedding for this project. This method requires a distance matrix to operate upon which can be obtained from the AS location data using the Haversine formula (discussed in section \ref{sec:haversine}). The method itself could be implemented in python, however an implementation already exists in MATLAB, which is the better choice as this implementation has been proven to work, and allows more development time for other parts of the project. 

\subsection{MATLAB in Python}

As this implementation is not native python and cannot be run as such, there are two choices for how to proceed. The simplest choice, though it is inelegant, is to export the distance matrix from python, import it into MATLAB, run the embedding procedure manually and transfer the results back. While it is simple, this method is extremely time consuming, as the manual transfer and execution significantly slows down runs of the overall application. Additionally, as this method is not automated it is possible that mistakes will be made when copying the data back and forth. 

Instead of manually transferring the data back and forth, it could be transferred automatically, by python, when the embedding procedure is required to be run. This solution eliminates the introduction of random error at the point of copying and decreases the running time of the application. Unfortunately, this method still requires the MATLAB embedding code to be invoked manually. There is, however, a third way. If it were possible to execute a MATLAB instance from inside python, and transfer data in and out of this instance then the entire solution could be completely automated.
Fortunately, a python library exists to solve this problem. A project named \textit{matlab\_wrapper} (\textit{github.com/mrkrd/matlab\_wrapper}) allows for the invoking of a MATLAB session from within python with functions to import and export data and execute arbitrary MATLAB code. With this library the entire embedding process can be automated, significantly speeding up the time required to produce visualisations. 

\subsection{Calculation of Embedding Points}

Though it is now possible to execute the code for the embedding, this does not produce the final embedding co-ordinates, only the inner product matrix, $\boldsymbol{Z}$, of the points. The steps to perform this conversion are discussed mathematically in section \ref{sec:hyperbolic_embedding} and will be discussed in terms of implementation here. 

The equation that must be implemented is:
\begin{equation}
\label{eq:embedding_x_impl}
\boldsymbol{X}=r\boldsymbol{U}_Z\boldsymbol{\Lambda}_{z||}^{\frac{1}{2}}
\end{equation}

where $r$ is the radius of the space, and $\boldsymbol{U}$ and $\boldsymbol{\Lambda}$ are the eigenvectors and eigenvalues of $\boldsymbol{Z}$ respectively. In order to implement this, the eigenvalues and eigenvectors of $\boldsymbol{Z}$ must be calculated. The linear algebra package of SciPy provides functionality to do this, and, as $\boldsymbol{Z}$ is symmetric (due to the inner product being commutative) it is possible to use the efficient \textit{eigh()} function designed for Hermitian or symmetric matrices. As an interesting side node, the standard \textit{eig()} function will not detect if a matrix is Hermitian or symmetric and will return complex values in any case. 

Having calculated the eigenvectors and eigenvalues of $\boldsymbol{Z}$ it is possible to calculate the value of $\boldsymbol{X}$ directly using equation \ref{eq:embedding_x_impl}. As the matrix values in this equation will be stored in NumPy arrays the implementation is simple, and the standard python multiplication function suffices. However, the standard python $\textit{sqrt()}$ does not support NumPy arrays and the NumPy equivalent must be used. As the magnitude of the eigenvalue matrix is required the NumPy \textit{absolute()} function must be used to prevent complex numbers being returned by \textit{sqrt()}.

Though $\boldsymbol{X}$ is able to be calculated, this does not yield the co-ordinates straight away. A further step is required in order to extract the correct dimensions. The eigenvalues of $\boldsymbol{Z}$ each correspond to a column in $\boldsymbol{X}$, this allows for the determination of the correct dimension columns as there will be a single large negative eigenvalue (the negative dimension of the space) and $N$ large positive eigenvalues (the positive dimensions of the space). All other eigenvalues should be have small magnitude compared to these. Finding the correct indexes for the dimensions is simple, the eigenvalue array must be sorted in ascending order. The eigenvalue, $\lambda^-$ for the negative dimension will then be at index 0 in the sorted array. The correct column in $\boldsymbol{X}$ can then be determined by finding the index of $\lambda^-$ in the eigenvalue matrix of $\boldsymbol{Z}$. The $N$ positive dimensions can be located in a similar fashion, though these will be located at index $-N$ for $N=1,...,n$, in the sorted eigenvalue array. It is then trivial to extract the correct columns from $\boldsymbol{X}$ and create co-ordinate tuples for each point in the embedding. For a three dimensional hyperbolic space four embedded dimensions are required as it is embedded in a higher dimensional space, the implications of this will be discussed in the next section.

\subsection{Conversion to the Ball Model}

Though it has not been discussed, there is a further model of hyperbolic space known as the hyperboloid model. This model represents $N$-dimensional hyperbolic space in an $(N+1)$-dimensional space, as discussed in section \ref{sec:hyperbolic_embedding}. Of course, the hyperbolic data in this project is in three dimensions, producing a four dimensional embedding, which is impossible to imagine in three dimensional space. Instead the embedding must be projected into three dimensional space in order to be able to produce meaningful visualisations. 

The Poincar\'{e} disc model, discussed earlier in section \ref{sec:discmodel} is a two dimensional representation of hyperbolic space, represented in two dimensional Euclidean space. There is a three dimensional version of this model, in which the disc is expanded into a sphere, and which represents three dimensional hyperbolic space. The model has similar behaviour as its two dimensional counterpart and is known as the \textit{Ball model}. Having the point data in this form would greatly simplify conversion to the other two models discussed in section \ref{sec:LitReviewVisHyperbolic}. 

In order to convert the four dimensional hyperboloid model to the three dimensional Ball model, it is necessary to project the higher dimensional space onto the lower one. In this case if the projection is through the point $(-1, 0, 0, 0)$ it will produce coordinates in the Ball model. This projection can be performed for a four dimensional point $\boldsymbol{x}$ as follows:

\begin{equation}
\frac{1}{x_0 + 1}
\begin{pmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{pmatrix}
\end{equation}

Implementing this in python is trivial using NumPy and looping through each co-ordinate. After the projection has been performed it is possible to plot the returned co-ordinates in three dimensional space. 

\section{Displaying Visualisations}
As the aim of this project is to produce visualisations it is necessary to perform some kind of 3D graphics programming. As co-ordinates in three dimensions have already been obtained the conversion to a visualisation should be trivial. A choice must be made, however, regarding the library to use for the graphics programming. 

There are multiple methods through which to produce a 3D visualisation in python, some of which are more flexible than others. If the points themselves are treated as generic data it is possible to use the python plotting library \textit{matplotlib} to produce a static plot of the data. This method is simple, but lacks flexibility, the plot cannot be rotated and the axes are fixed. Instead of using a method designed for visualising data, it would be better to treat the data as visual information and use a full 3D graphics library. 

There are many choices available for such 3D programming within python, one of which is VPython (\textit{vpython.org}). VPython is a python distribution that comes with the python module \textit{visual} pre installed. This module allows for flexible 3D graphics programming through an IDE named VIDLE. Though it may seem ideal, this solution is aimed at those wishing to experiment with this type of programming, being restricted to the IDE supplied with the distribution is not ideal as the application being produced does not only contain graphics programming. Further, there is no native Linux version of VPython and this project is being developed on Linux. 

Instead of concentrating on how to use python to produce visualisations it would be more sensible to seek to produce the best visualisation possible and then consider how to do this using python. VTK \textit{www.vtk.org} is an open-source software system for 3D computer graphics. VTK is written in C++ but has bindings in python and other languages. As it is compiled to native code it is fast and compatible with any operating system. This is a good choice for this project as it provides the flexibility required and enables the visualisation to be created directly from python.

The actual process of displaying visualisations will not be discussed here as it is effectively boilerplate and not interesting considering the domain of the project. 
